# 1
System: COE Cluster (iota)
model CPU: Intel(R) Xeon(R) CPU X5650 @ 2.67GHz
cores: 2 sockets, 6 core/socket, 2 threads/core
memory: 47GB
operating system: CentOs 7

## HPCG
1. 45.2548
2. 45.2565
3. 45.1685
4. 45.0477
5. 45.314
6. 64.7114 <- Significantly different
7. 45.212
8. 45.2815
9. 45.2651
10. 45.2562

Avg: 47.17677

Run number 6 took signifcantly longer than the rest of runs. This could be a
result of someone else logging into the system and contending for compute
resources by running another program. 

## CPU Benchmark
https://github.com/arihant15/Performance-Evaluation-Benchmark
(1,000,000 FLOP, 1 thread)
1.  0.210108
2.  0.209205
3.  0.209714
4.  0.208517
5.  0.209399
6.  0.209827
7.  0.208531
8.  0.209278
9.  0.209998
10. 0.209862
Avg: 0.2094439

Running with the -O3 flag enabled brought run time down to ~.0002 seconds. 
The O3 flag is short hand for a numer of optimization flags which help reduce
execution time at the expense of compile time and possibly binary size. The
increased binary size and aggressive compiler time "optimizations" actually 
resulted in a decrease in performance over the O2 flag which resulted in a run
time of .00006s. 

# 2

## Sample Sort Using PThreads to sort 5 million numbers
### Time to Sort
|   1 thread   |   2 thread  |   4 thread  |   8 thread   |
|--------------|-------------|-------------|--------------|
| 4.1s         | 3.77s       | 3.41s       | 3.39s        |

### Scaling Efficiency 
| Scaling  |   1 thread   |   2 thread  |   4 thread  |   8 thread   |
|----------|--------------|-------------|-------------|--------------|
| Strong   | 100%         | 54.37%      | 30.06%      |  15.12%      |
| Weak     | 100%         | 108.7%      | 120.2%      |  120.9%      |

Running on coe cluster (iota): 2 socket, 6 core/socket, 2 threads/core

Strong Scaling: fixed total problem size; Efficiency: (t1/N\*tN)\*100%
Weak Scaling: fixed problem size per processor; Efficiency (t1/tN)\*100%

This implementation does not have strong scaling capabilities. Running the sort
on a smaller sample size (e.g. 10,000), there is too little performance gain to
be seen by the timer. This is because the amount of work that can be parallelized
was too small compared to the total work done to sort the numbers.

The weak scaling capabilities of the system are much better in this case. We see
an increase in efficiency when we increase the cores from 1 to 2 to 4. The transition
between 4 and 8 cores is marked by a sharp drop in speedup difference. 
This drop is unsuprising because each CPU only has 6 cores; therefore, attempting
to spawn 8 threads will force data to be sent between CPUs  over the high speed
interconnect between sockets (if using 1 thread per core) OR cause two cores to
be oversubscribed, hidering performance of 4 threads (2 per core). 



# 3
hostname: c2130
model: Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz
caches: L1d cache:             32K      -> unique to each core
        L1i cache:             32K      -> unique to each core
        L2 cache:              256K     -> unique to each core
        L3 cache:              20480K   -> shared bewteen all cores
2 sockets, 8 core per socket, 2 thread per core
OS: CentOS 7 (`cat /etc/os-release`)
Network: 82599ES 10-Gigabit SFI/SFP+ Network Connection, Latency: ??? 

# 4
Trends: 
- significant dropoff in peak performance from top 3 to rest of the top 10
    - > 90 petaflops to ~20 petaflops (ignoring Tianhe2A @ 60 petaflops)
- the two top 10 Chinese based machines have significantly more cores and higher 
power consumption
- top 2 machines use IBM POWER9 CPUs with infinaband interconnect with fat tree 
  topology
- heterogenous architectures (2 sockets + accelerators)/node
    - NVIDIA V100: Summit (6), Sierra (4), ABCI (4))
    - NVIDIA P100: Piaz Daint (1)
    - Intel Xeon CoProcessors:Tianhe (3)
- 2 types of nodes
    - login nodes (low compute. just an entry point)
    - compute nodes (high power, run computation)
- interconnects
    - Infinaband interconnect
    - Intel Omni Path
    - Cray Aries interconnect
- Topologies
    - Fat Tree Topologies
    - Dragonfly 
- Utilize SSD's over hard disks

`https://abci.ai/en/about_abci/computing_resource.html`
