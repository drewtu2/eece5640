# Accelerating the Harris Corner Detector through Multithreading

In this project, I implemented a basic harris corner detector in C++ and then
accelerated it using OpenMP and CUDA. Image processing is an incredibly
SIMD problem, lending itself incredibly well to acceleration through multithreading. 

![Pipeline](pipeline.png)

This is because all of the operations in the pipeline are done for each component
pixel value, with each pixel value being performed as a stencil operation of the
surrouding pixels. This means there isn't a ton of complicated control logic 
involved and for the most part, processing is embarrisingly parallel.

## Verification
In order to verify my algorithm, I compared the output to the Harris Corner
Detector implmemented in OpenCV. While I don't expect the results to be exactly 
the same, the OpenCV method serves as a "dipstick" verfication. Specifically, 
my implementaiton doesn't do the Gaussian Smoothing and non max supression
usuallly found in the Harris Corner Detection. 

![Leopard Result](comparison.png)

## Speedups
The first method I used for speedup was OpenMP. Most of the loop iterations
in this algorithm involved iterating over pixel values. I started with a version
which serialized the data from a 2D array to a 1D array to improve memory
access speeds. The example below shows the calcualtion of gradients `ix` and 
`iy` from the input values. Each iteration of this loop is independent of one
another and as a result, it can immediately be parallelized through use of 
OpenMP.

### Calculating Gradients
```cpp
void HarrisCorner::calculate_gradients(float* ix, float* iy, float* input, int width, int height) {

    int x, y;
    int x_minus_one, x_plus_one;
    int y_minus_one, y_plus_one;
    float dx, dy;

    // Iterate over the entire image
    #pragma omp parallel for
    for(int ii = 0; ii < width * height; ++ii) {
        x = ii % width;
        y = ii / width;

        // Break out...
        if(x <= 1 || y <= 1 || x >= width - 2 || y >= height - 2) {
            continue;
        }

        x_minus_one = y*width + (x - 1);
        x_plus_one = y*width + (x + 1);
        y_minus_one = (y - 1)*width + x;
        y_plus_one = (y + 1)*width + x;

        dx = abs(input[x_minus_one] - input[x_plus_one]);
        dy = abs(input[y_minus_one] - input[y_plus_one]);

        ix[ii] = dx;
        iy[ii] = dy;
    }
}
```

This code is lends itself to CUDA. In CUDA, we spawn a single CUDA thread
per pixel. From the CUDA thread/block indices and dimensions, we can calculate
the x and y pixel position that each CUDA thread operates on. With this information
we can also calculate the neighboring "stencil" to find the image gradients. 

```cpp
_global__ void calculate_response(unsigned char *input,
                       float *output,
                       unsigned int height,
                       unsigned int width){

	int x = blockIdx.x*TILE_SIZE+threadIdx.x;
	int y = blockIdx.y*TILE_SIZE+threadIdx.y;

    float k = .04;

    // This represents the first and last row/column of the image. sobel needs
    // a delta of at least one
    if(x <= 1 || y <= 1 || x >= width - 2 || y >= height - 2) {
        return;
    }

    // calculate the offsets for all regions of interest
    int offset = (y * width) + x;
    int x_minus_one = y*width + (x - 1);
    int x_plus_one = y*width + (x + 1);
    int y_minus_one = (y - 1)*width + x;
    int y_plus_one = (y + 1)*width + x;

    // Calculate dx, dy
    float dx = input[x_minus_one] - input[x_plus_one];
    float dy = input[y_minus_one] - input[y_plus_one];

    // Caclualte dx2, dy2, dxy
    float ix2 = dx*dx;
    float iy2 = dy*dy;
    float ixy = dx*dy;

    // Harris Corner Response Matrix:
    // [ix2, ixy;
    // ixy, iy2]
    float itrace    = ix2 + iy2;
    float idet      = (ix2*iy2) - (ixy*ixy);

    float response = abs(idet - (k * itrace * itrace));

    output[offset] = response;
}
```

One of the most important things to consider when working with the GPUs is to
minimizes the number of data transfer back and forth between the GPU and the 
host machine. The result is a function that computes the entire Harris Corner 
Response in a single shot per thread as opposed to pipelining by calculating 
larger blocks at once  and combining at a higher level (i.e. computing all
`ix`, `iy` values, then computing all `ix^2`, `iy^2`, and `ixy`). 

Another important consideration is maximizing the "Compute per Global Memory 
Access" (CGMA) ratio. This is achievable by reading global memory into a local
variable and running from there. However, repeated accesses are still made to 
global memory as a result of the repeated access incurred by the stencil operations
done as part of the gradient computations. This could be further optimized by
performing a SINGLE read by each thread into shared memory and then further
accessing the memory from there. While some overhead is incurred by the 
synchronization of threads between the pre-load and execution, memory access as 
a whole would increase.

Unfortuantely I wasn't able to implement this feature before the 8am deadline 
due to lack of cluster access as a result of a user using all 8 GPUs on Saturday 
for several hours. 

## Timing Results

![Timing](timing_results.png)

The above graphic shows the timing results achieved by the OpenMP and CUDA
implementations. OpenMP had some interesting and incredibly vexing results. 
On one day, iterations with different OpenMP threads showed timing increasing by
100ms for each additional pair of threads being applied. These results don't quite
make sense since the additional threads should work will with the embarissingly
parallel problem. At the very least it should keep results consistent, not 
decrease them. On a different day, the same code yielded fairly consistent timings
across the board. 

More interesting is the resulting timing from the CUDA implementation. I ran 
the CUDA code with two different tile sizes: one with a 16x16 tile and another
with an 8x8 tile size. The tiles here referred to the number of threads per 
thread block in each dimension. With a 2D problem, I created 2D tiles. 

Here we see a timing of 150ms for the 16x16 block and ~200 ms for the 8x8 block. 
At first glance these results don't seem too much more impressive than the original
CPU implementation until you consider the breakdown of the timing. 

### 16x16 Tiling
![NVPROF 16x16 Timing](16x16.png "16x16 Timing")

### 8x8 Tiling
![NVPROF 8x8 Timing](8x8.png)

The function that computes the actual response (`calculate_response`) takes on
the order of ~177-300 us to run, an ordr of magnitude less than the CPU counterpart. 
This shows that while the computation itself is significantly faster, a steep
overhead is paid for setting up the problem. Namely, the `cudaMalloc` takes 
close to 97% of the time (~250ms). Looking more closely at the timing, it appears
`cudaMalloc` is called twice, with the min and max valus ranging WILDLY, from 
~250ms as the max to a min of ~220us. At the suggestion of one of my classmates
I tried "warming up" the GPU by calling a malloc outside the first timing test
to see what this would do to the results. The results were drastic. 

![Warmup](warmup.png)

By adding in the warm up routine to prior to timing the run of the GPU, the 
`runtime` of the GPU code dropped 12ms. Its important to note here that the runtime
itself did not *actually* change, just the timing of the run that did. This is
because the first call to the GPU takes a bit longer to "initialize" the GPU and 
bring all systems online. When we were running the benchmark, this meant that the
timing data for the first malloc was causing a higher run time. This is fixed 
when we "warmup" the program by allocating some memory (unused later on) prior
to beginning the actual computations. In essence, this experiment showed the 
importance of the warmup procedure when evaluating benchmarks. 

This is also extended to the CPUs which are run multiple times before the first
computation actually takes place. 

### Tiling

As part of my experiments, I explored how different sized tiles played an impact
on the timing of results. The previous two nvprof screen shots show the difference
in timing between different sized tiles. 

![Warmup](tile_size.png)

The size of the tile ultimately refers to how many threads are being launched
per block running on a stream multiprocessor (SM). With larger tiles, fewer
blocks of more threads are launched while with smaller tiles, more blocks with
fewer threads are launch. This results in differences in caching since oversubscribing
the number of blocks results in more context switches between blocks which can
introduce an overhead. 

A tradeoff to consider (in future implementations) is the size of shared memory
associated to each block for different block sizes. Since multiple blocks can 
run on a single SM but each SM only has a finite amount of block size, the 
amount of shared memory each block can get is directly related to the number of
blocks being launched. 

## Future Work
There were a number of components that I wanted to explore for this project
that I unfortunatley was not able to explore. 

### Shared memory: 
For improving the timing on a single image, the use of shared memory would
have been one of the most interesting investigaitons to exploit existing 
parallelism. While shared memory is significantly samller than global memory,
it is also significantly faster to access. 

### Asynchronous Access: 
Asynchronous access could enable the simulataneous access and processing of data 
resulting in up to 2x speedups over the current implementation. This speedup comes
from the ability of the GPU to begin processing data as soon as it arrives rather
than waiting for all data to arrive before beginning the process it. However, 
this is dont by taking advantage of the rastorized nature of data transfers - 
data is sent row by row. For certain computations which require the blendeing
of data in different columns AND rows, temporal constraints dictate that a row
must have completed transfer before processing can begin. As a result, some 
algorithms may need to be revised in order to accomodate this optimization. 

### Multiple GPUs: 
Multiple GPUS can become useul in splitting up problems even further to obtain a
higher degree of parallelism. My intuition says for a single image the benefits
of using mutliple GPUs may not outweight the overhead in coordination and that
this sort of solution is likely better suited for the larger problem sizes.

### 3D data (Multiple pictures)
While note an additional optimization, increasing the problem size would provide
a better sense o how his algorithms scales across a number of nodes. Temporal
data (such as sucessive frames in a video) can be incredibly compute heavy 
depending on the rate/size of the problem and will be able to further take 
advantage of the parallel hardware to solve the problems. 

# Code
All code is publically available on GitHub at the following link. 
https://github.com/drewtu2/eece5640/tree/master/final_project/harris
